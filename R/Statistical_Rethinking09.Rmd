---
title: "Statistical_Rethinking09"
author: "Robert A. Stevens"
date: "May 4, 2016"
output: html_document
---

**TODO:**

1. Format R code

2. Run R code and check

3. Do practice problems

```{r, comment=NA}
library(rethinking)
library(StanHeaders)
```

*Statistical Rethinking: A Bayesian Course with Examples in R and Stan*

by Richard McElreath

# 9.0 Big Entropy and the Generalized Linear Model  

Rethinking: Bayesian updating and maximum entropy

## 9.1 Maximum entropy  

Figure 9.1. Entropy as a measure of the number of unique arrangements of a system that produce the same distribution. Plots A through E show the numbers of unique wasy to arrange 10 pebbles into each of 5 different distributions. Bottom-right: The entropy of each distribution plotted against the log number of ways per pebble to produce it.

```{r, comment=NA}
p <- list()
p$A <- c(0,0,10,0,0)
p$B <- c(0,1,8,1,0)
p$C <- c(0,2,6,2,0)
p$D <- c(1,2,4,2,1)
p$E <- c(2,2,2,2,2)
```

```{r, comment=NA}
p_norm <- lapply( p , function(q) q/sum(q))
```

```{r, comment=NA}
( H <- sapply( p_norm , function(q) -sum(ifelse(q==0,0,q*log(q))) ) )
```

```{r, comment=NA}
ways <- c(1,90,1260,37800,113400)
logwayspp <- log(ways)/10
```

Rethinking: What good is intuition?

Overthinking: The Wallis derivation

### 9.1.1 Gaussian

Figure 9.2. Maximum entropy and the Gaussian distribution. Left: Comparison of Gaussian (blue) and several other continuous distributions with the same variance. Right: Entropy is maximized when curvature of a generalized normal distribution matches the Gaussian, where shape is equal to 2.

Overthinking: Proof of Gaussian maximum entropy

### 9.1.2 Binomial

Figure 9.3. Four different distributions with the same expected value, 1 blue marble in 2 draws. The outcomes on the horizontal axes correspond to 2 white marbles (ww), 1 blue and then 1 white (bw), 1 white and then 1 blue (wb, and 2 blue marbles (bb).

```{r, comment=NA}
# build list of the candidate distributions
p <- list()
p[[1]] <- c(1/4,1/4,1/4,1/4)
p[[2]] <- c(2/6,1/6,1/6,2/6)
p[[3]] <- c(1/6,2/6,2/6,1/6)
p[[4]] <- c(1/8,4/8,2/8,1/8)

# compute expected value of each
sapply( p , function(p) sum(p*c(0,1,1,2)) )
```

```{r, comment=NA}
# compute entropy of each distribution
sapply( p , function(p) -sum( p*log(p) ) )
```

```{r, comment=NA}
p <- 0.7
( A <- c( (1-p)^2 , p*(1-p) , (1-p)*p , p^2 ) )
```

```{r, comment=NA}
-sum( A*log(A) )
```

```{r, comment=NA}
sim.p <- function(G=1.4) {
    x123 <- runif(3)
    x4 <- ( (G)*sum(x123)-x123[2]-x123[3] )/(2-G)
    z <- sum( c(x123,x4) )
    p <- c( x123 , x4 )/z
    list( H=-sum( p*log(p) ) , p=p )
}
```

```{r, comment=NA}
H <- replicate( 1e5 , sim.p(1.4) )
dens( as.numeric(H[1,]) , adj=0.1 )
```

```{r, comment=NA}
entropies <- as.numeric(H[1,])
distributions <- H[2,]
```

```{r, comment=NA}
max(entropies)
```

```{r, comment=NA}
distributions[ which.max(entropies) ]
```

Figure 9.4. Left: Distribution of entropies from randomly simulated distributions with expected value 1.4. The letters A, B, C, and D mark the entropies of individual distributions show on the right. Right: Individual probability distributions. As entropy decreases, going from A to D, the distribution becomes more uneven. The distribution marked A is the binomial distribution with np = 1.4.

Rethinking: Conditional independence

Overthinking: Binomial maximum entropy

## 9.2 Generalized linear models  

Figure 9.5. Why we need link functions. The solid blue line is a linear model of a probability mass. It increases linearly with a predictor, x, on the horizontal axis. But when it reaches the maximum probability mass of 1, at the dashed boundary, it will happily continue upwards, as show by the dashed blue line. In reality, further increases in x could not further increase probability, as indicated by the horizontal continuation of the solid trend.

Rethinking: The scourge of Histomancy

### 9.2.1 Meet the family

Figure 9.6. Some of the exponential family distributions, their notation, and some of their relationships. Center: exponential distribution. Clockwise, from top-left: gamma, normal (Gaussian), binomial and Poisson distributions.

Rethinking: A likelihood is a prior

### 9.2.2 Linking linear models to distributions

Figure 9.7. The logit link transforms a linear model (left) into a probability (right). This transformation compresses the geometry far from zero, such that a unit change on the linear scale (left) means less and less change on the probability scale (right).

Figure 9.8. The log link transforms a linear model (left) into a strictly positative measurement (right). This transform results in an exponential scaling of the linear model, with a unit change on the linear scale mapping onto increasingly larger changes on the outcome scale.

Rethinking: When in doubt, play with assumptions

Overthinking: Parameters interacting with themselves

### 9.2.3 Absolute and relative differences

### 9.2.4 GLMs and information criteria

## 9.3 Maximum entropy priors  

## 9.4 Summary  
