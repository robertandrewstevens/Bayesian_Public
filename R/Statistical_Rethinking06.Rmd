---
title: "Statistical_Rethinking06"
author: "Robert A. Stevens"
date: "May 4, 2016"
output: html_document
---

*Statistical Rethinking: A Bayesian Course with Examples in R and Stan*

by Richard McElreath

**TODO:**

1. Run R code and check

2. Do practice problems

```{r, comment=NA}
library(rethinking)
library(StanHeaders)
```

# 6.0 Overfitting, Regularization, and Information Criteria  

Figure 6.1. Ptolemaic (left) and Copernican (right) models of the solar system. Both models use epicycles (circles on circles), and both models produce exactly the sample predictions. However, the Copernican model requires fewer circles. (Not all Ptolemaic epicycles are visible in the figure.)

Rethinking: Stargazing.

Rethinking: Is AIC Bayesian?

## 6.1 The problem with parameters  

### 6.1.1 More parameters always improve fit

Figure 6.2. Average brain volume in cubic centimeters against body mass in kilograms, for six hominin species. What model best describes the relationship between brain size and body size?

```{r, comment=NA}
sppnames <- c("afarensis", "africanus", "habilis", "boisei", "rudolfensis", "ergaster", "sapiens")
brainvolcc <- c(438, 452, 612, 521, 752, 871, 1350)
masskg <- c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5)
d <- data.frame(species = sppnames, brain = brainvolcc, mass = masskg)
```

```{r, comment=NA}
m6.1 <- lm(brain ~ mass, data = d)
```

```{r, comment=NA}
1 - var(resid(m6.1))/var(d$brain)
```

```{r, comment=NA}
m6.2 <- lm(brain ~ mass + I(mass^2), data = d)
```

```{r, comment=NA}
m6.3 <- lm(brain ~ mass + I(mass^2) + I(mass^3)                                     , data = d)
m6.4 <- lm(brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4)                         , data = d)
m6.5 <- lm(brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5)             , data = d)
m6.6 <- lm(brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5) + I(mass^6) , data = d)
```

Figure 6.3. Polynomial linear models of increasing degree, fit to the hominin data. Each plot shows the predicted mean in black, with 89% interval of the mean shaded. R^2, is displayed above each plot. (a) First-degree polynomial. (b) Second-degree. (c) Third-degree. (d) Fourth-degree. (e) Fifth-degree. (f) Sixth-degree.

Rethinking: Model fitting as compression

### 6.1.2 Too few parameters hurts, too

```{r, comment=NA}
m6.7 <- lm(brain ~ 1, data = d)
```

Figure 6.4. An underfit model of hominin brain volume. This model ignores any association between body mass and brain volume, producing a horizontal line of predictions. As a result, the model fits badly and (presumably) predicts badly.

Figure 6.5. Underfitting and overfitting as under-sensitivity and over-sensitivity to sample. In both plots, a regression is fit to the seven sets of data made by dropping one row from the original data. (a) An underfit model is insensitive to the sample, changing as individual points are dropped. (b) An overfit model is sensitive to the sample, changing dramatically as points are dropped.

Overthinking: Dropping rows

```{r, comment=NA}
d.new <- d[-i, ]
```

```{r, comment=NA}
plot(brain ~ mass, d, col = "slateblue")
for(i in 1:nrow(d)) {
  d.new <- d[-i, ]
  m0 <- lm(brain ~ mass, d.new)
  abline(m0, col = col.alpha("black", 0.5))
}
```

Rethinking: Bias and variance

## 6.2 Information theory and model performance  

### 6.2.1 Firing the weatherperson

#### 6.2.1.1 Costs and benefits

#### 6.2.1.2 Measuring accuracy

Rethinking: What is a true model?

### 6.2.2 Information and uncertainty

```{r, comment=NA}
p <- c(0.3, 0.7)
-sum(p*log(p))
```

Overthinking: More on entropy

Rethinking: The benefits of maximizing uncertainty

### 6.2.3 From entropy to accuracy

Figure 6.6 Information divergence of an approximating distribution q from a true distribution p. Divergence can only equal zero when q = p (dashed line). Otherwise, the divergence is positive and grow as q becomes more dissimilar from p. When we have more than one candidate approximation q, the q with the smallest divergence is the most accurate approximation, in the sense that it induces the least additional uncertainty.

Overthinking: Cross entropy and divergence

Rethinking: Divergence depends upon direction

### 6.2.4 From divergence to deviance

```{r, comment=NA}
# fit model with lm
m6.1 <- lm(brain ~ mass, d)

# compute deviance by cheating
(-2)*logLik(m6.1)
```

Overthinking: Computing deviance

```{r, comment=NA}
# standardize the mass before fitting
#d$mass.s <- (d$mass - mean(d$mass))/sd(d$mass)
d$mass.s <- with(d, (mass - mean(mass))/sd(mass))
m6.8 <- map(alist(
              brain ~ dnorm(mu, sigma),
              mu <- a + b*mass.s), 
            data = d,
            start = list(a = mean(d$brain), b = 0, sigma = sd(d$brain)),
            method = "Nelder-Mead")

# extract MAP estimates
theta <- coef(m6.8)

# compute deviance
dev <- (-2)*sum(dnorm(d$brain,
                      mean = theta[1] + theta[2]*d$mass.s,
                      sd = theta[3],
                      log = TRUE))
dev
```

### 6.2.5 From deviance to out-of-sample

Figure 6.7. Deviance in and out of sample. In each plot, models with different numbers of predictor variables are shown on the horizontal axis. Deviance across 10,000 simulations is show on the vertical. Blue shows deviance in-sample, the training data. Black shows deviance out-of-sample, the test data. Points show means, and the line segments show +/- 1 standard deviation.

Overthinking: Simulated training and testing

```{r, comment=NA}
N <- 20
kseq <- 1:5
dev <- sapply(kseq, 
              function(k) {
                print(k)
                r <- replicate(1e4, sim.train.test(N = N, k = k))
                c(mean(r[1, ]), mean(r[2, ]), sd(r[1, ]), sd(r[2, ]))
              } 
)
```

```{r, comment=NA}
r <- mcreplicate(1e4, sim.train.test(N = N, k = k), mc.cores = 4)
```

```{r, comment=NA}
plot(1:5, dev[1, ], pch = 16, col = rangi2, 
     xlim = c(1, 5.1), ylim = c(min(dev[1:2, ]) - 5, max(dev[1:2, ]) + 10), 
     xlab = "number of parameters", ylab = "deviance")
mtext(concat("N = ", N))
points((1:5) + 0.1, dev[2, ])
for(i in kseq) {
  pts_in  <- dev[1, i] + c(-1, +1)*dev[3, i]
  pts_out <- dev[2, i] + c(-1, +1)*dev[4, i]
  lines(c(i, i), pts_in, col = rangi2)
  lines(c(i, i) + 0.1, pts_out)
}
```

## 6.3 Regularization  

Figure 6.8. Regularizing priors, weak and strong. Three Gaussian priors of varying standard deviation. These priors reduce overfitting, but with different strength. Dashed: Normal(0, 1). Thin solid: Normal(0, 0.5). Thick solid: Normal(0, 0.2).

Figure 6.9. Regularizing priors and out-of-sample deviance. The points in both plots are the same as in Figure 6.7. The lines show training (blue) and testing (black) deviance for the three regularizing priors in Figure 6.8. Dashed: Each beta-coefficient is given a Normal(0, 1) prior. Thin solid: Normal(0, 0.5). Thick solid: Normal(0, 0.2).

Rethinking: Multilevel models as adaptive regularization

Rethinking: Ridge regression

## 6.4 Information criteria  

Figure 6.10. Deviance in-sample (blue) and out-of-sample (black), using flat priors. The vertical segments measure the distance between each pair of deviance. For both N = 20 and N = 100, this distance is approximately twice the number of parameters. The dashed lines show exactly the deviance in-sample (training) plus twice the number of parameters on the horizontal axis. These lines therefore show AIC for each model, an approximation of the out-of-sample deviance.

Rethinking: Information criteria and consistency

### 6.4.1 DIC

### 6.4.2 WAIC

Rethinking: What about BIC?

Overthinking: WAIC calculations

```{r, comment=NA}
data(cars)
m <- map(alist(
           dist ~ dnorm(mu, sigma),
           mu <- a + b*speed,
           a ~ dnorm(0, 100),
           b ~ dnorm(0, 10),
           sigma ~ dunif(0, 30)), 
         data = cars)
post <- extract.samples(m, n = 1000)
```

```{r, comment=NA}
n_samples <- 1000
ll <- sapply(1:n_samples,
             function(s) {
               mu <- post$a[s] + post$b[s]*cars$speed
               dnorm(cars$dist, mu, post$sigma[s], log = TRUE)
             })
```

```{r, comment=NA}
n_cases <- nrow(cars)
lppd <- sapply(1:n_cases, function(i) log_sum_exp(ll[i, ]) - log(n_samples))
```

```{r, comment=NA}
pWAIC <- sapply( 1:n_cases, function(i) var(ll[i, ]))
```

```{r, comment=NA}
-2*(sum(lppd) - sum(pWAIC))
```

```{r, comment=NA}
waic_vec <- -2*(lppd - pWAIC)
sqrt(n_cases*var(waic_vec))
```

### 6.4.3 DIC and WAIC as estimates of deviance

Figure 6.11. Out-of-sample deviance as estimated by DIC and WAIC. Points are average out-of-sample deviance over 10,000 simulations. The lines are average DIC (top) and WAIC (bottom) computed from the same simulations. The black points and lines come from simulations with a nearly flat Normal(0, 100) prior. The blue points and lines used a regularizing Normal(0, 0.5) prior.

Rethinking: Diverse prediction framework

## 6.5 Using information criteria  

### 6.5.1 Model comparison

```{r, comment=NA}
data(milk)
d <- milk[complete.cases(milk), ]
d$neocortex <- d$neocortex.perc/100
dim(d)
```

```{r, comment=NA}
a.start <- mean(d$kcal.per.g)
sigma.start <- log(sd(d$kcal.per.g))
m6.11 <- map(alist(
               kcal.per.g ~ dnorm(a, exp(log.sigma))),
             data = d, 
             start = list(a = a.start, log.sigma = sigma.start))
m6.12 <- map(alist(
               kcal.per.g ~ dnorm(mu, exp(log.sigma)),
               mu <- a + bn*neocortex),
             data = d,
             start = list(a = a.start, bn = 0, log.sigma = sigma.start))
m6.13 <- map(alist(
               kcal.per.g ~ dnorm(mu, exp(log.sigma)),
               mu <- a + bm*log(mass)),
             data = d, 
             start = list(a = a.start, bm = 0, log.sigma = sigma.start))
m6.14 <- map(alist(
               kcal.per.g ~ dnorm(mu , exp(log.sigma)),
               mu <- a + bn*neocortex + bm*log(mass)),
             data = d, 
             start = list(a = a.start, bn = 0, bm = 0, log.sigma = sigma.start))
```

#### 6.5.1.1 Comparing WAIC values

```{r, comment=NA}
WAIC(m6.14)
```

```{r, comment=NA}
(milk.models <- compare(m6.11, m6.12, m6.13, m6.14))
```

```{r, comment=NA}
plot(milk.models, SE = TRUE, dSE = TRUE)
```

```{r, comment=NA}
diff <- rnorm(1e5, 6.7, 7.26)
sum(diff < 0)/1e5
```

Rethinking: How big a difference in WAIC is "significant"?

Rethinking: WAIC metaphors

#### 6.5.1.2 Comparing estimates

```{r, comment=NA}
coeftab(m6.11, m6.12, m6.13, m6.14)
```

```{r, comment=NA}
plot(coeftab(m6.11, m6.12, m6.13, m6.14))
```

Figure 6.12. Comparing the posterior densities of parameters for the four model fits to the primate milk data. Each point is a MAP estimate, and each black line segment is a 89% percentile interval. Estimates are grouped by parameter identify, and each row in a group is a model.

Rethinking: Barplots suck

### 6.5.2 Model averaging

```{r, comment=NA}
# compute counterfactual predictions
# neocortex from 0.5 to 0.8
nc.seq <- seq(from = 0.5, to = 0.8, length.out = 30)
d.predict <- list(kcal.per.g = rep(0, 30), # empty outcome
                  neocortex = nc.seq,      # sequence of neocortex
                  mass = rep(4.5, 30)      # average mass
)
pred.m6.14 <- link(m6.14, data = d.predict)
mu <- apply(pred.m6.14, 2, mean)
mu.PI <- apply(pred.m6.14, 2, PI)

# plot it all
plot(kcal.per.g ~ neocortex, d, col = rangi2)
lines(nc.seq, mu       , lty = 2)
lines(nc.seq, mu.PI[1,], lty = 2)
lines(nc.seq, mu.PI[2,], lty = 2)
```

Figure 6.13. Model averaged posterior predictive distribution for the primate milk analysis. The dashed regression line and dashed 89% percentile interval correspond to the minimum-WAIC model, m6.14. The solid line and shaded 89% region correspond to the model averaged predictions.

```{r, comment=NA}
milk.ensemble <- ensemble(m6.11, m6.12, m6.13, m6.14, data =d.predict)
mu <- apply(milk.ensemble$link, 2, mean)
mu.PI <- apply(milk.ensemble$link, 2, PI)
lines(nc.seq, mu)
shade(mu.PI, nc.seq)
```

Rethinking: The Curse of Tippecanoe

## 6.6 Summary  

## 6.7 Practice  

6E1

6E2

6E3

6E4

6M1

6M2

6M3

6M4

6M5

6M6

```{r, comment=NA}
library(rethinking)
data(Howell1)
d <- Howell1
#d$age <- (d$age - mean(d$age))/sd(d$age)
d$age <- with(d, (age - mean(age))/sd(age))
set.seed(1000)
i <- sample(1:nrow(d), size = nrow(d)/2)
d1 <- d[ i, ]
d2 <- d[-i, ]
```

6H1

6H2

6H3

6H4

```{r, comment=NA}
sum(dnorm(d2$height, mu, sigma, log = TRUE))
```

6H5

6H6
