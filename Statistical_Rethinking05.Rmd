---
title: "Statistical_Rethinking05"
author: "Robert A. Stevens"
date: "May 4, 2016"
output: html_document
---

**TODO:**

1. Run R code and check

2. Do practice problems

```{r, comment=NA}
library(rethinking)
library(StanHeaders)
```

*Statistical Rethinking: A Bayesian Course with Examples in R and Stan*

by Richard McElreath

# 5.0 Multivariate Linear Models  

Figure 5.1. the number of Waffle House diners per million people is associated with divorce rate (in the year 2009) within the United States. Each point is a State. “Southern” (former Confederate) States shown in blue. Shaded region is 89% percentile interval of the mean. These data are in data(waffleDivorice) in the rethinking package.

Rethinking: Causal inference.

## 5.1 Spurious association  

Figure 5.2. Divorce rate is associated with both marriage rate (left) and median age marriage (right). Both predictor variables are standardized in this example. The average marriage rate across States is 20 per 1000 adults, and the average median age at marriage is 26 years.

```{r, comment=NA}
# load data
library(rethinking)
data(WaffleDivorce)
d <- WaffleDivorce

# standardize predictor
#d$MedianAgeMarriage.s <- (d$MedianAgeMarriage - mean(d$MedianAgeMarriage))/sd(d$MedianAgeMarriage)
d$MedianAgeMarriage.s <- with(d, (MedianAgeMarriage - mean(MedianAgeMarriage))/sd(MedianAgeMarriage))

# fit model
m5.1 <- map(
          alist(
            Divorce ~ dnorm(mu, sigma),
            mu <- a + bA*MedianAgeMarriage.s,
            a ~ dnorm(10, 10),
            bA ~ dnorm(0, 1),
            sigma ~ dunif(0, 10)
          ), data = d)
```

```{r, comment=NA}
# compute percentile interval of mean
MAM.seq <- seq(from = -3, to = 3.5, length.out = 30)
mu <- link(m5.1, data = data.frame(MedianAgeMarriage.s = MAM.seq))
mu.PI <- apply(mu, 2, PI)

# plot it all
plot(Divorce ~ MedianAgeMarriage.s, data = d, col = rangi2)
abline(m5.1)
shade(mu.PI, MAM.seq)
```

```{r, comment=NA}
#d$Marriage.s <- (d$Marriage - mean(d$Marriage))/sd(d$Marriage)
d$Marriage.s <- with(d, (Marriage - mean(Marriage))/sd(Marriage))
m5.2 <- map(
          alist(
            Divorce ~ dnorm(mu, sigma),
            mu <- a + bR*Marriage.s,
            a ~ dnorm(10, 10),
            bR ~ dnorm(0, 1),
            sigma ~ dunif(0, 10)
          ), data = d)
```

Rethinking: "Control" is out of control

### 5.1.1 Multivariate notation

Overthinking: Compact notation and the design matrix

### 5.1.2 Fitting the model

```{r, comment=NA}
m5.3 <- map(
          alist(
            Divorce ~ dnorm(mu, sigma),
            mu <- a + bR*Marriage.s + bA*MedianAgeMarriage.s,
            a ~ dnorm(10, 10),
            bR ~ dnorm(0, 1),
            bA ~ dnorm(0, 1),
            sigma ~ dunif(0, 10)
          ), data = d)
precis( m5.3 )
```

```{r, comment=NA}
plot(precis(m5.3))
```

### 5.1.3 Plotting multivariate posteriors

#### 5.1.3.1 Predictor residual plots

```{r, comment=NA}
m5.4 <- map(
          alist(
            Marriage.s ~ dnorm(mu, sigma),
            mu <- a + b*MedianAgeMarriage.s,
            a ~ dnorm(0, 10),
            b ~ dnorm(0, 1),
            sigma ~ dunif(0, 10)
          ), data = d)
```

```{r, comment=NA}
# compute expected value at MAP, for each State
mu <- coef(m5.4)['a'] + coef(m5.4)['b']*d$MedianAgeMarriage.s
# compute residual for each State
m.resid <- d$Marriage.s - mu
```

```{r, comment=NA}
plot(Marriage.s ~ MedianAgeMarriage.s, d, col = rangi2)
abline(m5.4)
# loop over States
for (i in 1:length(m.resid)) {
  x <- d$MedianAgeMarriage.s[i] # x location of line segment
  y <- d$Marriage.s[i] # observed endpoint of line segment
  # draw the line segment
  lines(c(x, x), c(mu[i], y), lwd = 0.5, col = col.alpha("black", 0.7))
}
```

Figure 5.3. Residual marriage rate in each State, after accounting for the linear association with median age at marriage. Each gray line segment is a residual, the distance of each observed marriage rate from the expected value, attempting to predict marriage rate with median age at marriage alone. So States that lie above the black regression line have higher rates of marriage than expected, according to age at marriage. Those below the line have lower rates than expected.

Figure 5.4. Predictor residual plots for the divorce data. Left: States with fast marriage rates for their median age of marriage have about the same divorce rates as do States with slow marriage rates. Right: States with old median age of marriage for their marriage rate have lower divorce rates, while States with young median age of marriage have higher divorce rates.

#### 5.1.3.2 Counterfactual plots

```{r, comment=NA}
# prepare new counterfactual data
A.avg <- mean(d$MedianAgeMarriage.s)
R.seq <- seq(from = -3, to = 3, length.out = 30)
pred.data <- data.frame(Marriage.s = R.seq, MedianAgeMarriage.s = A.avg)

# compute counterfactual mean divorce (mu)
mu <- link(m5.3, data = pred.data)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply( mu, 2, PI)

# simulate counterfactual divorce outcomes
R.sim <- sim(m5.3, data = pred.data, n = 1e4)
R.PI <- apply(R.sim, 2, PI)

# display predictions, hiding raw data with type="n"
plot(Divorce ~ Marriage.s, data = d, type = "n")
mtext("MedianAgeMarriage.s = 0")
lines(R.seq, mu.mean)
shade(mu.PI, R.seq)
shade(R.PI , R.seq)
```

Figure 5.5. Counterfactual plots for the multivariate divorce model, m5.3. Each plot shows the chagne in predicted mean across values of a single predictor, holding the other predictor constant at its mean value (zero in both cases). Shaded regions show 89% percentile intervals of the mean (dark, narrow) and 89% prediction intervals (light, wide).

```{r, comment=NA}
R.avg <- mean(d$Marriage.s)
A.seq <- seq(from = -3, to = 3.5, length.out = 30)
pred.data2 <- data.frame(Marriage.s = R.avg, MedianAgeMarriage.s = A.seq)

mu <- link(m5.3, data = pred.data2)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply( mu, 2, PI)

A.sim <- sim(m5.3, data = pred.data2, n = 1e4)
A.PI <- apply(A.sim, 2, PI)

plot(Divorce ~ MedianAgeMarriage.s, data = d, type = "n")
mtext("Marriage.s = 0")
lines(A.seq, mu.mean)
shade(mu.PI, A.seq)
shade(A.PI, A.seq)
```

#### 5.1.3.3 Posterior prediction plots

```{r, comment=NA}
# call link without specifying new data
# so it uses original data
mu <- link(m5.3)

# summarize samples across cases
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

# simulate observations
# again no new data, so uses original data
divorce.sim <- sim(m5.3, n = 1e4)
divorce.PI <- apply(divorce.sim, 2, PI)
```

```{r, comment=NA}
plot(mu.mean ~ d$Divorce, col = rangi2, ylim = range(mu.PI),
     xlab = "Observed divorce", ylab = "Predicted divorce")
abline(a = 0, b = 1, lty = 2)
for(i in 1:nrow(d))
  lines(rep(d$Divorce[i], 2) , c(mu.PI[1, i], mu.PI[2, i]), col = rangi2)
```

Figure 5.6. Posterior predictive plots for the multivariate divorce model, m5.3. (a) Predicted divorce rate against observed, with 89% confidence intervals of the average prediction. The dashed line shows perfect prediction. (b) Average prediction error for each State, with 89% interval of the mean (black line) and 89% interval of the mean error (residuals) against number of Waffle House per capita, with superimposed regression of the two variables.

```{r, comment=NA}
identify(x = d$Divorce, y = mu.mean , labels = d$Loc, cex = 0.8)
```

```{r, comment=NA}
# compute residuals
divorce.resid <- d$Divorce - mu.mean
# get ordering by divorce rate
o <- order(divorce.resid)
# make the plot
dotchart(divorce.resid[o], labels = d$Loc[o], xlim = c(-6, 5), cex = 0.6)
abline(v = 0, col = col.alpha("black", 0.2))
for(i in 1:nrow(d)) {
  j <- o[i] # which State in order
  lines(d$Divorce[j] - c(mu.PI[1, j], mu.PI[2, j]), rep(i, 2))
  points(d$Divorce[j] - c(divorce.PI[1, j], divorce.PI[2, j]), rep(i, 2), pch = 3, cex = 0.6, col = "gray")
}
```

Rethinking: Stats, huh, yeah what is good for?

Overthinking: Simulating spurious association

```{r, comment=NA}
N <- 100                           # number of cases
x_real <- rnorm(N)                 # x_real as Gaussian with mean 0 and stddev 1
x_spur <- rnorm(N , x_real)        # x_spur as Gaussian with mean=x_real
y <- rnorm(N, x_real)              # y as Gaussian with mean=x_real
d <- data.frame(y, x_real, x_spur) # bind all together in data frame
```

## 5.2 Masked relationship  

```{r, comment=NA}
library(rethinking)
data(milk)
d <- milk
str(d)
```

```{r, comment=NA}
m5.5 <- map(
          alist(
            kcal.per.g ~ dnorm(mu, sigma),
            mu <- a + bn*neocortex.perc,
            a ~ dnorm(0, 100),
            bn ~ dnorm(0, 1),
            sigma ~ dunif(0, 1)
          ), data = d)
```

```{r, comment=NA}
d$neocortex.perc
```

```{r, comment=NA}
dcc <- d[complete.cases(d), ]
```

```{r, comment=NA}
m5.5 <- map(
          alist(
            kcal.per.g ~ dnorm(mu, sigma),
            mu <- a + bn*neocortex.perc,
            a ~ dnorm(0, 100),
            bn ~ dnorm(0, 1),
            sigma ~ dunif(0, 1)
          ), data = dcc)
```

```{r, comment=NA}
precis(m5.5, digits = 3)
```

```{r, comment=NA}
coef(m5.5)["bn"] * (76 - 55)
```

```{r, comment=NA}
np.seq <- 0:100
pred.data <- data.frame(neocortex.perc = np.seq)

mu <- link(m5.5, data = pred.data, n = 1e4)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

plot(kcal.per.g ~ neocortex.perc, data = dcc, col = rangi2)
lines(np.seq, mu.mean)
lines(np.seq, mu.PI[1, ], lty = 2)
lines(np.seq, mu.PI[2, ], lty = 2)
```

```{r, comment=NA}
dcc$log.mass <- log(dcc$mass)
```

```{r, comment=NA}
m5.6 <- map(
          alist(
            kcal.per.g ~ dnorm(mu, sigma),
            mu <- a + bm*log.mass,
            a ~ dnorm(0, 100),
            bm ~ dnorm( 0, 1),
            sigma ~ dunif(0, 1)
          ), data = dcc)
precis(m5.6)
```

Figure 5.7. Milk energy and neocortex among primates. In the top two plots, simple bivariate regressions of kilocalories per gram of milk on (left) neocortex percent and (right) log female body mass how wear and uncertain associations. However, on the bottom, a single regression with both neocortex percent and log body mass suggests strong association with both variables. Both neocortex and body mass are associate with milk energy, but in opposite directions. This masks each variable's relationship with the outcome, unless both are considered simultaneously.

```{r, comment=NA}
m5.7 <- map(
          alist(
            kcal.per.g ~ dnorm(mu, sigma),
            mu <- a + bn*neocortex.perc + bm*log.mass,
            a ~ dnorm( 0, 100),
            bn ~ dnorm(0, 1),
            bm ~ dnorm(0, 1),
            sigma ~ dunif(0, 1)
          ), data = dcc)
precis(m5.7)
```

```{r, comment=NA}
mean.log.mass <- mean(log(dcc$mass))
np.seq <- 0:100
pred.data <- data.frame(neocortex.perc = np.seq, log.mass = mean.log.mass)

mu <- link(m5.7, data = pred.data, n = 1e4)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

plot(kcal.per.g ~ neocortex.perc, data = dcc, type = "n")
lines(np.seq, mu.mean)
lines(np.seq, mu.PI[1, ], lty = 2)
lines(np.seq, mu.PI[2, ], lty = 2)
```

Overthinking: Simulating a masking relationship

```{r, comment=NA}
N <- 100                                      # number of cases
rho <- 0.7                                    # correlation btw x_pos and x_neg
x_pos <- rnorm(N)                             # x_pos as Gaussian
x_neg <- rnorm(N, rho*x_pos, sqrt(1 - rho^2)) # x_neg correlated with x_pos
y <- rnorm(N, x_pos - x_neg)                  # y equally associated with x_pos, x_neg
d <- data.frame(y, x_pos, x_neg)              # bind all together in data frame
```

## 5.3 When adding variables hurts  

### 5.3.1 Multicollinear legs

```{r, comment=NA}
N <- 100                                         # number of individuals
height <- rnorm(N, 10, 2)                        # sim total height of each
leg_prop <- runif(N, 0.4, 0.5)                   # leg as proportion of height
leg_left  <- leg_prop*height + rnorm(N, 0, 0.02) # sim left leg as proportion + error
leg_right <- leg_prop*height + rnorm(N, 0, 0.02) # sim right leg as proportion + error
d <- data.frame(height, leg_left, leg_right)     # combine into data frame
```

```{r, comment=NA}
m5.8 <- map(
          alist(
            height ~ dnorm(mu, sigma),
            mu <- a + bl*leg_left + br*leg_right,
            a ~ dnorm(10, 100),
            bl ~ dnorm(2, 10),
            br ~ dnorm(2, 10),
            sigma ~ dunif(0, 10)
          ), data = d)
precis(m5.8)
```

```{r, comment=NA}
plot(precis(m5.8))
```

```{r, comment=NA}
post <- extract.samples(m5.8)
plot(bl ~ br, post, col = col.alpha(rangi2, 0.1), pch = 16)
```

Figure 5.8. Left: Posterior distribution of the association of each leg with height, from model m5.8. Since both variables contain almost identical information, the posterior is a narrow ridge of negatively correlated values. Right: The posterior distribution of the sum of the two parameters is centered on the proper association of either leg with height.

```{r, comment=NA}
sum_blbr <- post$bl + post$br
dens(sum_blbr, col = rangi2, lwd = 2, xlab = "sum of bl and br")
```

```{r, comment=NA}
m5.9 <- map(
          alist(
            height ~ dnorm(mu, sigma),
            mu <- a + bl*leg_left,
            a ~ dnorm(10, 100),
            bl ~ dnorm(2, 10),
            sigma ~ dunif(0, 10)
          ), data = d)
precis(m5.9)
```

### 5.3.2 Multicollinear milk

```{r, comment=NA}
library(rethinking)
data(milk)
d <- milk
```

```{r, comment=NA}
# kcal.per.g regressed on perc.fat
m5.10 <- map(
           alist(
             kcal.per.g ~ dnorm(mu, sigma),
             mu <- a + bf*perc.fat,
             a ~ dnorm(0.6, 10),
             bf ~ dnorm(0, 1),
             sigma ~ dunif(0, 10)
          ), data = d)

# kcal.per.g regressed on perc.lactose
m5.11 <- map(
           alist(
             kcal.per.g ~ dnorm(mu, sigma),
             mu <- a + bl*perc.lactose,
             a ~ dnorm(0.6, 10),
             bl ~ dnorm(0, 1),
             sigma ~ dunif(0, 10)
          ), data = d)

precis(m5.10, digits = 3)
precis(m5.11, digits = 3)
```

```{r, comment=NA}
m5.12 <- map(
           alist(
             kcal.per.g ~ dnorm(mu, sigma),
             mu <- a + bf*perc.fat + bl*perc.lactose,
             a ~ dnorm(0.6, 10),
             bf ~ dnorm(0, 1),
             bl ~ dnorm(0, 1),
             sigma ~ dunif(0, 10)
           ), data = d)
precis(m5.12, digits = 3)
```

```{r, comment=NA}
pairs(~ kcal.per.g + perc.fat + perc.lactose, data = d, col = rangi2)
```

Figure 5.9. A pairs plot of the total energy, percent fat, and percent lactose variables from the primate milk data. Percent fat and percent lactose are strongly negatively correlated with one another, providing mostly the same information.

```{r, comment=NA}
cor(d$perc.fat, d$perc.lactose)
```

Figure 5.10. The effect of correlated predictor variables on the narrowness of the posterior distribution. The vertical axis shows the standard deviation of the posterior distribution of the slope. The horizontal axis is the correlation between the predictor of interest and another predictor added to the model. As the correlation increases, the standard deviation inflates.

Rethinking: Identification guaranteed, comprehension up to you

Overthinking: Simulating collinearity

```{r, comment=NA}
library(rethinking)
data(milk)
d <- milk
sim.coll <- function(r = 0.9) {
  d$x <- rnorm(nrow(d), mean = r*d$perc.fat, sd = sqrt((1 - r^2)*var(d$perc.fat)))
  m <- lm(kcal.per.g ~ perc.fat + x, data = d)
  sqrt(diag(vcov(m)))[2] # stddev of parameter
}
rep.sim.coll <- function(r = 0.9, n = 100) {
    stddev <- replicate(n, sim.coll(r))
    mean(stddev)
}
r.seq <- seq(from = 0, to = 0.99, by = 0.01)
stddev <- sapply(r.seq, function(z) rep.sim.coll(r = z, n = 100))
plot(stddev ~ r.seq, type = "l", col = rangi2, lwd = 2, xlab = "correlation")
```

### 5.3.3 Post-treatment bias

```{r, comment=NA}
# number of plants
N <- 100

# simulate initial heights
h0 <- rnorm(N, 10, 2)

# assign treatments and simulate fungus and growth
treatment <- rep(0:1, each = N/2)
fungus <- rbinom(N, size = 1, prob = 0.5 - treatment*0.4)
h1 <- h0 + rnorm(N, 5 - 3*fungus)

# compose a clean data frame
d <- data.frame(h0 = h0, h1 = h1, treatment = treatment, fungus = fungus)
```

```{r, comment=NA}
m5.13 <- map(
           alist(
             h1 ~ dnorm(mu, sigma),
             mu <- a + bh*h0 + bt*treatment + bf*fungus,
             a ~ dnorm(0, 100),
             c(bh, bt, bf) ~ dnorm(0, 10),
             sigma ~ dunif(0, 10)
           ), data = d)
precis(m5.13)
```

```{r, comment=NA}
m5.14 <- map(
           alist(
             h1 ~ dnorm(mu, sigma),
             mu <- a + bh*h0 + bt*treatment,
             a ~ dnorm(0, 100),
             c(bh, bt) ~ dnorm(0, 10),
             sigma ~ dunif(0, 10)
           ), data = d)
precis(m5.14)
```

Rethinking: Model comparison doesn't help

## 5.4 Categorical variables  

### 5.4.1 Binary categories

```{r, comment=NA}
data(Howell1)
d <- Howell1
str(d)
```

```{r, comment=NA}
m5.15 <- map(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu <- a + bm*male ,
        a ~ dnorm( 178 , 100 ) ,
        bm ~ dnorm( 0 , 10 ) ,
        sigma ~ dunif( 0 , 50 )
    ) ,
    data=d )
precis(m5.15)
```

```{r, comment=NA}
post <- extract.samples(m5.15)
mu.male <- post$a + post$bm
PI(mu.male)
```

Overthinking: Re-parameterizing the model

```{r, comment=NA}
m5.15b <- map(
            alist(
              height ~ dnorm(mu, sigma),
              mu <- af*(1 - male) + am*male,
              af ~ dnorm(178, 100),
              am ~ dnorm(178, 100),
              sigma ~ dunif(0, 50)
            ), data = d)
```

### 5.4.2 Many categories

```{r, comment=NA}
data(milk)
d <- milk
unique(d$clade)
```

```{r, comment=NA}
(d$clade.NWM <- ifelse(d$clade == "New World Monkey", 1, 0))
```

```{r, comment=NA}
d$clade.OWM <- ifelse(d$clade == "Old World Monkey", 1, 0)
d$clade.S   <- ifelse(d$clade == "Strepsirrhine"   , 1, 0)
```

```{r, comment=NA}
m5.16 <- map(
           alist(
             kcal.per.g ~ dnorm(mu, sigma),
             mu <- a + b.NWM*clade.NWM + b.OWM*clade.OWM + b.S*clade.S,
             a ~ dnorm(0.6, 10),
             b.NWM ~ dnorm(0, 1),
             b.OWM ~ dnorm(0, 1),
             b.S ~ dnorm(0, 1),
             sigma ~ dunif(0, 10)
           ), data = d)
precis(m5.16)
```

```{r, comment=NA}
# sample posterior
post <- extract.samples(m5.16)

# compute averages for each category
mu.ape <- post$a
mu.NWM <- post$a + post$b.NWM
mu.OWM <- post$a + post$b.OWM
mu.S   <- post$a + post$b.S

# summarize using precis
precis(data.frame(mu.ape, mu.NWM, mu.OWM, mu.S))
```

```{r, comment=NA}
diff.NWM.OWM <- mu.NWM - mu.OWM
quantile(diff.NWM.OWM, probs = c(0.025, 0.500, 0.975))
```

Rethinking: Differences and statistical significance

### 5.4.3 Adding regular predictor variables.

### 5.4.4 Another approach: Unique intercepts

```{r, comment=NA}
(d$clade_id <- coerce_index(d$clade))
```

```{r, comment=NA}
m5.16_alt <- map(
               alist(
                 kcal.per.g ~ dnorm(mu, sigma),
                 mu <- a[clade_id],
                 a[clade_id] ~ dnorm(0.6, 10),
                 sigma ~ dunif(0, 10)
               ), data = d)
precis(m5.16_alt, depth = 2)
```

## 5.5 Ordinary least squares and lm  

### 5.5.1 Design formulas

### 5.5.2 Using lm

```{r, comment=NA}
m5.17 <- lm(y ~ 1 + x        , data = d)
m5.18 <- lm(y ~ 1 + x + z + w, data = d)
```

#### 5.5.2.1 Intercepts are optional

```{r, comment=NA}
m5.17 <- lm(y ~ 1 + x, data = d)
m5.19 <- lm(y ~ x    , data = d)
```

```{r, comment=NA}
m5.20 <- lm(y ~ 0 + x, data = d)
m5.21 <- lm(y ~ x - 1, data = d)
```

#### 5.5.2.2 Categorical variables

```{r, comment=NA}
m5.22 <- lm(y ~ 1 + as.factor(season), data = d)
```

#### 5.5.2.3 Transform variables first

```{r, comment=NA}
d$x2 <- d$x^2
d$x3 <- d$x^3
m5.23 <- lm(y ~ 1 + x + x2 + x3, data = d)
```

```{r, comment=NA}
m5.24 <- lm(y ~ 1 + x + I(x^2) + I(x^3), data = d)
```

#### 5.5.2.4 No estimate for sigma

### 5.5.3 Building map formulas from lm formulas

```{r, comment=NA}
data(cars)
glimmer(dist ~ speed, data = cars)
```

## 5.6 Summary  

## 5.7 Practice  

5E1

5E2

5E3

5E4

5M1

5M2

5M3

5M4

5M5

5H1

5H2

5H3
