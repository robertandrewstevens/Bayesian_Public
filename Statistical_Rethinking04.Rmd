---
title: "Statistical_Rethinking04"
author: "Robert A. Stevens"
date: "May 4, 2016"
output: html_document
---

**TODO:**

1. Run R code and check

2. Do practice problems

```{r, comment=NA}
library(rethinking)
library(StanHeaders)
```

*Statistical Rethinking: A Bayesian Course with Examples in R and Stan*

by Richard McElreath

# 4.0 Linear Models  

Figure 4.1. The Ptolemaic Universe, in which complex motion of the planets in the night sky was explained by orbits within orbits, called epicycles. The model is incredibly wrong, yet makes quite good predictions.

## 4.1 Why normal distributions are normal  

### 4.1.1 Normal by addition

```{r, comment=NA}
pos <- replicate(1000, sum(runif(16, -1, 1)))
```

Figure 4.2. Random walks on the soccer field converge to a normal distribution. The more steps are taken, the closer the match between the real empirical distribution of positions and the ideal normal distribution, superimposed in the last plot in the bottom panel.

### 4.1.2 Normal by multiplication

```{r, comment=NA}
prod(1 + runif(12, 0, 0.1))
```

```{r, comment=NA}
growth <- replicate(10000, prod(1 + runif(12, 0, 0.1)))
dens(growth, norm.comp = TRUE)
```

```{r, comment=NA}
big <- replicate(10000, prod(1 + runif(12, 0, 0.5)))
small <- replicate(10000, prod(1 + runif(12, 0, 0.01)))
```

### 4.1.3 Normal by log-multiplication

```{r, comment=NA}
log.big <- replicate(10000, log(prod(1 + runif(12, 0, 0.5))))
```

### 4.1.4 Using Gaussian distributions

#### 4.1.4.1 Ontological justification

#### 4.1.4.2 Epistemological justification

Overthinking: Gaussian distribution

## 4.2 A language for describing models  

### 4.2.1 Rd-describing the globe tossing model

Overthinking: From model definition to Bayes' theorem

```{r, comment=NA}
w <- 6
n <- 9
p_grid <- seq(from = 0, to = 1, length.out = 100)
posterior <- dbinom(w, n, p_grid)*dunif(p_grid, 0, 1)
posterior <- posterior/sum(posterior)
```

## 4.3 A Gaussian model of height  

### 4.3.1 The data

```{r, comment=NA}
library(rethinking)
data(Howell1)
d <- Howell1
```

```{r, comment=NA}
str(d)
```

```{r, comment=NA}
d$height
```

Overthinking: Data frames

```{r, comment=NA}
d2 <- d[d$age >= 18, ]
```

Overthinking: Index magic

### 4.3.2 The model

Rethinking: Independent and identically distributed

```{r, comment=NA}
curve(dnorm(x, 178, 20), from = 100, to = 250)
```

```{r, comment=NA}
curve(dunif(x, 0, 50), from = -10, to = 60)
```

Rethinking: A farewell to epsilon

Overthinking: Model definition to Bayes' theorem again

R code 4.13 ????
```{r, comment=NA}
sample_mu <- rnorm(1e4, 178, 20)
sample_sigma <- runif(1e4, 0, 50)
prior_h <- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_h)
```

### 4.3.3 Grid approximation of the posterior distribution

```{r, comment=NA}
mu.list <- seq(from = 140, to = 160, length.out = 200)
sigma.list <- seq(from = 4, to = 9, length.out = 200)
post <- expand.grid(mu = mu.list, sigma = sigma.list)
post$LL <- sapply(1:nrow(post), 
                  function(i) 
                    sum(dnorm(d2$height, mean = post$mu[i], sd = post$sigma[i], log = TRUE)))
post$prod <- post$LL + dnorm(post$mu, 178, 20, TRUE) + dunif(post$sigma, 0, 50, TRUE)
post$prob <- exp(post$prod - max(post$prod))
```

```{r, comment=NA}
contour_xyz(post$mu, post$sigma, post$prob)
```

```{r, comment=NA}
image_xyz(post$mu, post$sigma, post$prob)
```

### 4.3.4 Sampling from the posterior

```{r, comment=NA}
sample.rows <- sample(1:nrow(post), size = 1e4, replace = TRUE, prob = post$prob)
sample.mu <- post$mu[sample.rows]
sample.sigma <- post$sigma[sample.rows]
```

```{r, comment=NA}
plot(sample.mu, sample.sigma, cex = 0.5, pch = 16, col = col.alpha(rangi2, 0.1))
```

```{r, comment=NA}
dens(sample.mu)
dens(sample.sigma)
```

```{r, comment=NA}
HPDI(sample.mu)
HPDI(sample.sigma)
```

Overthinking: Sample size and the normality of sigma's posterior

```{r, comment=NA}
d3 <- sample(d2$height, size = 20)
```

```{r, comment=NA}
mu.list <- seq(from = 150, to = 170, length.out = 200)
sigma.list <- seq(from = 4, to = 20, length.out = 200)
post2 <- expand.grid(mu = mu.list, sigma = sigma.list)
post2$LL <- sapply(1:nrow(post2), 
                   function(i)
                     sum(dnorm(d3, mean = post2$mu[i], sd = post2$sigma[i], log = TRUE)))
post2$prod <- post2$LL + dnorm(post2$mu, 178, 20, TRUE) + dunif(post2$sigma, 0, 50, TRUE)
post2$prob <- exp(post2$prod - max(post2$prod))
sample2.rows <- sample(1:nrow(post2), size = 1e4, replace = TRUE, prob=post2$prob)
sample2.mu <- post2$mu[sample2.rows]
sample2.sigma <- post2$sigma[sample2.rows]
plot(sample2.mu, sample2.sigma, cex = 0.5, col = col.alpha(rangi2, 0.1),
     xlab = "mu", ylab = "sigma", pch = 16)
```

```{r, comment=NA}
dens(sample2.sigma, norm.comp = TRUE)
```

### 4.3.5 Fitting the model with map

```{r, comment=NA}
library(rethinking)
data(Howell1)
d <- Howell1
d2 <- d[d$age >= 18, ]
```

```{r, comment=NA}
flist <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178, 20),
  sigma ~ dunif(0, 50)
)
```

```{r, comment=NA}
m4.1 <- map(flist, data = d2)
```

```{r, comment=NA}
precis(m4.1)
```

Overthinking: Start values for map

```{r, comment=NA}
start <- list(
  mu = mean(d2$height),
  sigma = sd(d2$height)
)
```

```{r, comment=NA}
m4.2 <- map(
          alist(
            height ~ dnorm(mu, sigma),
            mu ~ dnorm(178, 0.1),
            sigma ~ dunif(0, 50)
          ),
          data = d2)
precis(m4.2)
```

Overthinking: How strong is a prior?

### 4.3.6 Sampling from a map fit

```{r, comment=NA}
vcov(m4.1)
```

```{r, comment=NA}
diag(vcov(m4.1))
cov2cor(vcov(m4.1))
```

```{r, comment=NA}
library(rethinking)
post <- extract.samples(m4.1, n = 1e4)
head(post)
```

```{r, comment=NA}
precis(post)
```

Overthinking: Under the hood with multivariate sampling

```{r, comment=NA}
library(MASS)
post <- mvrnorm(n = 1e4, mu = coef(m4.1), Sigma = vcov(m4.1))
```

Overthinking: Getting sigma right

```{r, comment=NA}
m4.1_logsigma <- map(
                   alist(
                     height ~ dnorm(mu, exp(log_sigma)),
                     mu ~ dnorm(178, 20),
                     log_sigma ~ dnorm(2, 10)
                   ), data = d2)
```

```{r, comment=NA}
post <- extract.samples(m4.1_logsigma)
sigma <- exp(post$log_sigma)
```

## 4.4 Adding a predictor  

```{r, comment=NA}
plot(d2$height ~ d2$weight)
```

Rethinking: What is "regression"

### 4.4.1 The linear model strategy

#### 4.4.1.1 Likelihood

#### 4.4.1.2 Linear model

Rethinking:  Nothing special or natural about linear models

Overthinking: Units and regression models

#### 4.4.1.3 Priors

Rethinking: What's the correct prior?

### 4.4.2 Fitting the model

```{r, comment=NA}
# load data again, since it's a long way back
library(rethinking)
data(Howell1)
d <- Howell1
d2 <- d[d$age >= 18, ]

# fit model
m4.3 <- map(
          alist(
            height ~ dnorm(mu, sigma),
            mu <- a + b*weight,
            a ~ dnorm(156, 100),
            b ~ dnorm(0, 10),
            sigma ~ dunif(0, 50)
          ), data = d2)
```

Rethinking: Everything that depends upon parameters has a posterior distribution.

Overthinking: Embedding linear models

```{r, comment=NA}
m4.3 <- map(
          alist(
            height ~ dnorm(a + b*weight, sigma),
            a ~ dnorm(178, 100),
            b ~ dnorm(0, 10),
            sigma ~ dunif(0, 50)
          ), data = d2)
```

### 4.4.3 Interpreting the model fit

Rethinking: What do parameters mean?

#### 4.4.3.1 Tables of estimates

```{r, comment=NA}
precis(m4.3)
```

```{r, comment=NA}
precis(m4.3, corr = TRUE)
```

```{r, comment=NA}
d2$weight.c <- d2$weight - mean(d2$weight)
```

```{r, comment=NA}
m4.4 <- map(
          alist(
            height ~ dnorm(mu, sigma),
            mu <- a + b*weight.c,
            a ~ dnorm(178, 100),
            b ~ dnorm(0, 10),
            sigma ~ dunif(0, 50)
          ), data = d2)
```

```{r, comment=NA}
precis(m4.4, corr = TRUE)
```

#### 4.4.3.2 Plotting posterior inference against the data

```{r, comment=NA}
plot(height ~ weight, data = d2)
abline(a = coef(m4.3)["a"], b = coef(m4.3)["b"])
```

#### 4.4.3.3 Adding uncertainty around the mean

Figure 4.4. Height in centimeteres (vertical) plotted agains weight in kilograms (horizontal), with the maximum a posteriori line for the mean height at each weight plotted in black.

```{r, comment=NA}
post <- extract.samples(m4.3)
```

```{r, comment=NA}
post[1:5, ]
```

```{r, comment=NA}
N <- 10
dN <- d2[1:N, ]
mN <- map(
        alist(
          height ~ dnorm(mu, sigma),
          mu <- a + b*weight,
          a ~ dnorm(178, 100),
          b ~ dnorm(0, 10),
          sigma ~ dunif(0, 50)
        ), data = dN)
```

```{r, comment=NA}
# extract 20 samples from the posterior
post <- extract.samples(mN, n = 20)

# display raw data and sample size
plot(dN$weight, dN$height,
     xlim = range(d2$weight), ylim = range(d2$height),
     col = rangi2, xlab = "weight", ylab = "height")
mtext(concat("N = ", N))

# plot the lines, with transparency
for(i in 1:20)
  abline(a = post$a[i], b = post$b[i], col = col.alpha("black", 0.3))
```

Figure 4.5. Samples from the quadratic approximate posterior distribution for the height/weight model, m4.3, with increasing amounts of data. In each plot, 20 lines sampled fro the posterior distribution, showing the uncertainty in the regression relationship. 

#### 4.4.3.4 Plotting regression intervals and contours

```{r, comment=NA}
mu_at_50 <- post$a + post$b*50
```

```{r, comment=NA}
dens(mu_at_50, col = rangi2, lwd = 2, xlab = "mu|weight = 50")
```

Figure 4.6. The quadratic approximate posterior distribution of the mean height, m, where weight is 50 kg. This distribution represents the relative plausibility of different values or the mean.

```{r, comment=NA}
HPDI(mu_at_50, prob = 0.89)
```

```{r, comment=NA}
mu <- link(m4.3)
str(mu)
```

```{r, comment=NA}
# define sequence of weights to compute predictions for
# these values will be on the horizontal axis
weight.seq <- seq(from = 25, to = 70, by = 1)

# use link to compute mu
# for each sample from posterior
# and for each weight in weight.seq
mu <- link(m4.3, data = data.frame(weight = weight.seq))
str(mu)
```

```{r, comment=NA}
# use type = "n" to hide raw data
plot(height ~ weight, d2, type = "n")

# loop over samples and plot each mu value
for(i in 1:100)
  points(weight.seq, mu[i, ], pch = 16, col = col.alpha(rangi2, 0.1))
```

Figure 4.7. Left: The first 100 value sin the distribution of m at each weight value. Right: The !Kung height data again, now with 89% HPDI of the mean indicated by the shaded region. Compare this region to the distributions of blue points on the left.

```{r, comment=NA}
# summarize the distribution of mu
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob = 0.89)
```

```{r, comment=NA}
# plot raw data
# fading out points to make line and interval more visible
plot(height ~ weight, data = d2, col = col.alpha(rangi2, 0.5))

# plot the MAP line, aka the mean mu for each weight
lines(weight.seq, mu.mean)

# plot a shaded region for 89% HPDI
shade(mu.HPDI, weight.seq)
```

Rethinking: Overconfident confidence intervals

Overthinking: How link works

```{r, comment=NA}
post <- extract.samples(m4.3)
mu.link <- function(weight) post$a + post$b*weight
weight.seq <- seq(from = 25, to = 70, by = 1)
mu <- sapply(weight.seq, mu.link)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob = 0.89)
```

#### 4.4.3.5 Prediction intervals

```{r, comment=NA}
sim.height <- sim(m4.3, data = list(weight = weight.seq))
str(sim.height)
```

```{r, comment=NA}
height.PI <- apply(sim.height, 2, PI, prob = 0.89)
```

```{r, comment=NA}
# plot raw data
plot(height ~ weight, d2, col = col.alpha(rangi2, 0.5))

# draw MAP line
lines(weight.seq, mu.mean)

# draw HPDI region for line
shade(mu.HPDI, weight.seq)

# draw PI region for simulated heights
shade(height.PI, weight.seq)
```

Figure 4.8. 89% prediction interval for height, as a function of weight. The solid line is the MAP estimate of the mean height at each weight. The tow shaded regions show different 89% plausible regions. The narrow shaded interval around the line is the distribution of m. The wider shaded region represents the region within which the model expects to find 89% of actual heights in the population, at each weight.

```{r, comment=NA}
sim.height <- sim(m4.3, data = list(weight = weight.seq), n = 1e4)
height.PI <- apply(sim.height, 2, PI, prob = 0.89)
```

Rethinking: Two kinds of uncertainty

Overthinking: Rolling your own sim.

```{r, comment=NA}
post <- extract.samples(m4.3)
weight.seq <- 25:70
sim.height <- sapply(weight.seq , 
                     function(weight)
                       rnorm(n = nrow(post), mean = post$a + post$b*weight, sd = post$sigma))
height.PI <- apply(sim.height, 2, PI, prob = 0.89)
```

## 4.5 Polynomial regression  

```{r, comment=NA}
library(rethinking)
data(Howell1)
d <- Howell1
str(d)
```

Rethinking: Linear, additive, funky.

```{r, comment=NA}
d$weight.s <- (d$weight - mean(d$weight))/sd(d$weight)
```

```{r, comment=NA}
d$weight.s2 <- d$weight.s^2
m4.5 <- map(
          alist(
            height ~ dnorm(mu, sigma),
            mu <- a + b1*weight.s + b2*weight.s2,
            a ~ dnorm(178, 100),
            b1 ~ dnorm(0, 10),
            b2 ~ dnorm(0, 10),
            sigma ~ dunif(0, 50)
          ), data = d)
```

```{r, comment=NA}
precis(m4.5)
```

```{r, comment=NA}
weight.seq <- seq(from = -2.2, to = 2, length.out = 30)
pred_dat <- list(weight.s = weight.seq, weight.s2 = weight.seq^2)
mu <- link(m4.5, data = pred_dat)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.89)
sim.height <- sim(m4.5, data = pred_dat)
height.PI <- apply(sim.height, 2, PI, prob = 0.89)
```

```{r, comment=NA}
plot(height ~ weight.s, d, col = col.alpha(rangi2, 0.5))
lines(weight.seq, mu.mean)
shade(mu.PI, weight.seq)
shade(height.PI, weight.seq)
```

Figure 4.9. Polynomial regression of height on weight (standardized), for the full !Kung data. In each plot, the raw data are show by the circles. The solid curves show the path of m in each model, and the shaded regions show the 89% interval of the mean (close to the solid curve) and the 89% interval of predictions (wider). (a) Linear regression. (b) A second order polynomial, a parabolic regression. (c) A third order polynomial, a cubic regression.

```{r, comment=NA}
d$weight.s3 <- d$weight.s^3
m4.6 <- map(
          alist(
            height ~ dnorm(mu, sigma),
            mu <- a + b1*weight.s + b2*weight.s2 + b3*weight.s3,
            a ~ dnorm(178, 100),
            b1 ~ dnorm(0, 10),
            b2 ~ dnorm(0, 10),
            b3 ~ dnorm(0, 10),
            sigma ~ dunif(0, 50)
          ), data = d)
```

Overthinking: Converting back to natural scale.

```{r, comment=NA}
plot(height ~ weight.s, d, col = col.alpha(rangi2, 0.5), xaxt = "n")
```

```{r, comment=NA}
at <- c(-2, -1, 0, 1, 2)
labels <- at*sd(d$weight) + mean(d$weight)
axis(side = 1, at = at, labels = round(labels,1))
```

## 4.6 Summary  

## 4.7 Practice  

4E1

4E2

4E3

4E4

4E5

4M1

4M2

4M3

4M4

4M5

4M6

4H1

4H2

4H3

```{r, comment=NA}
plot(height ~ weight, data = Howell1, col = col.alpha(rangi2, 0.4))
```
